{"cells":[{"cell_type":"code","execution_count":null,"id":"4ca48608-7d96-48ca-8d94-c73c987283d1","metadata":{},"outputs":[],"source":["# Installing required packages\n","\n","!pip install wget pyspark  findspark"]},{"cell_type":"markdown","id":"3e94540c-4025-4062-8977-7e9cbc4a388a","metadata":{},"source":["#### Prework - Initiate the Spark Session\n"]},{"cell_type":"code","execution_count":1,"id":"5f6d4999-e4c7-4b14-b26f-2a097b8d88bd","metadata":{},"outputs":[],"source":["import findspark\n","\n","findspark.init()"]},{"cell_type":"code","execution_count":2,"id":"f2cb94fb-1fb3-470b-ae8b-b182c6a787be","metadata":{},"outputs":[],"source":["# PySpark is the Spark API for Python. we use PySpark to initialize the SparkContext.   \n","\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession\n","import wget"]},{"cell_type":"code","execution_count":3,"id":"85087614-bd16-43b4-9a97-857389058458","metadata":{},"outputs":[],"source":["# Creating a SparkContext object\n","\n","sc = SparkContext.getOrCreate()\n","\n","# Creating a Spark Session\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark DataFrames basic example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Data From Csv"]},{"cell_type":"code","execution_count":4,"id":"5e18a115-3875-4635-8078-90568d04e5ef","metadata":{},"outputs":[],"source":["#load the data into a pyspark dataframe\n","df1=spark.read.csv('purchases.csv',header=True,inferSchema=True)\n","df2=spark.read.csv('transactions.csv',header=True,inferSchema=True)"]},{"cell_type":"markdown","id":"733d1bb2-6f14-4ca5-aa5f-2eb0367c7373","metadata":{},"source":["###  Display the schema of both dataframes\n","\n"]},{"cell_type":"code","execution_count":5,"id":"20de2783-5c4a-452b-8aef-0e98295c6e96","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- customer_id: integer (nullable = true)\n"," |-- date_column: string (nullable = true)\n"," |-- amount: integer (nullable = true)\n"," |-- description: string (nullable = true)\n"," |-- location: string (nullable = true)\n","\n","root\n"," |-- customer_id: integer (nullable = true)\n"," |-- transaction_date: string (nullable = true)\n"," |-- value: integer (nullable = true)\n"," |-- notes: string (nullable = true)\n","\n"]}],"source":["#print the schema of df1 and df2\n","df1.printSchema()\n","df2.printSchema()\n"]},{"cell_type":"markdown","id":"686d6e94-7abb-4a51-af9a-618a7dbf9ae4","metadata":{},"source":["#### Add a new column to each dataframe\n","\n"]},{"cell_type":"code","execution_count":18,"id":"a04d44eb-d483-4876-897e-723a95b1316e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+------+-----------+--------+----+\n","|customer_id|date_column|amount|description|location|year|\n","+-----------+-----------+------+-----------+--------+----+\n","|          1|   1/1/2022|  5000| Purchase A| Store A|2022|\n","|          2|  15/2/2022|  1200| Purchase B| Store B|2022|\n","|          3|  20/3/2022|   800| Purchase C| Store C|2022|\n","|          4|  10/4/2022|  3000| Purchase D| Store D|2022|\n","|          5|   5/5/2022|  6000| Purchase E| Store E|2022|\n","|          6|  10/6/2022|  4500| Purchase F| Store F|2022|\n","|          7|  15/7/2022|   200| Purchase G| Store G|2022|\n","|          8|  20/8/2022|  3500| Purchase H| Store H|2022|\n","|          9|  25/9/2022|   700| Purchase I| Store I|2022|\n","|         10| 30/10/2022|  1800| Purchase J| Store J|2022|\n","|         11|  5/11/2022|  2200| Purchase K| Store K|2022|\n","|         12| 10/12/2022|   900| Purchase L| Store L|2022|\n","|         13|  15/1/2023|  4800| Purchase M| Store M|2023|\n","|         14|  20/2/2023|   300| Purchase N| Store N|2023|\n","|         15|  25/3/2023|  4200| Purchase O| Store O|2023|\n","|         16|  30/4/2023|  2600| Purchase P| Store P|2023|\n","|         17|   5/5/2023|   700| Purchase Q| Store Q|2023|\n","|         18|  10/6/2023|  1500| Purchase R| Store R|2023|\n","|         19|  15/7/2023|  3200| Purchase S| Store S|2023|\n","|         20|  20/8/2023|  1000| Purchase T| Store T|2023|\n","+-----------+-----------+------+-----------+--------+----+\n","only showing top 20 rows\n","\n","+-----------+----------------+-----+-------+-------+\n","|customer_id|transaction_date|value|  notes|quarter|\n","+-----------+----------------+-----+-------+-------+\n","|          1|        1/1/2022| 1500| Note 1|      1|\n","|          2|       15/2/2022| 2000| Note 2|      1|\n","|          3|       20/3/2022| 1000| Note 3|      1|\n","|          4|       10/4/2022| 2500| Note 4|      2|\n","|          5|        5/5/2022| 1800| Note 5|      2|\n","|          6|       10/6/2022| 1200| Note 6|      2|\n","|          7|       15/7/2022|  700| Note 7|      3|\n","|          8|       20/8/2022| 3000| Note 8|      3|\n","|          9|       25/9/2022|  600| Note 9|      3|\n","|         10|      30/10/2022| 1200|Note 10|      4|\n","|         11|       5/11/2022| 1500|Note 11|      4|\n","|         12|      10/12/2022|  800|Note 12|      4|\n","|         13|       15/1/2023| 2000|Note 13|      1|\n","|         14|       20/2/2023|  700|Note 14|      1|\n","|         15|       25/3/2023| 1800|Note 15|      1|\n","|         16|       30/4/2023| 1000|Note 16|      2|\n","|         17|        5/5/2023|  400|Note 17|      2|\n","|         18|       10/6/2023| 1500|Note 18|      2|\n","|         19|       15/7/2023| 3000|Note 19|      3|\n","|         20|       20/8/2023|  600|Note 20|      3|\n","+-----------+----------------+-----+-------+-------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import year, quarter, to_date\n","\n","spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n","\n","#Add new column year to df1\n","df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n","df1.show()\n","\n","#Add new column quarter to df2    \n","df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))\n","df2.show()\n"]},{"cell_type":"markdown","id":"1e7d24db-6589-468f-9a5d-59e05154b028","metadata":{},"source":["####  Rename columns in both dataframes\n","\n"]},{"cell_type":"code","execution_count":22,"id":"d1e15886-3a62-45a7-b34a-80cc3d7363a8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+------------------+-----------+--------+----+\n","|customer_id|date_column|transaction_amount|description|location|year|\n","+-----------+-----------+------------------+-----------+--------+----+\n","|          1|   1/1/2022|              5000| Purchase A| Store A|2022|\n","|          2|  15/2/2022|              1200| Purchase B| Store B|2022|\n","|          3|  20/3/2022|               800| Purchase C| Store C|2022|\n","|          4|  10/4/2022|              3000| Purchase D| Store D|2022|\n","|          5|   5/5/2022|              6000| Purchase E| Store E|2022|\n","|          6|  10/6/2022|              4500| Purchase F| Store F|2022|\n","|          7|  15/7/2022|               200| Purchase G| Store G|2022|\n","|          8|  20/8/2022|              3500| Purchase H| Store H|2022|\n","|          9|  25/9/2022|               700| Purchase I| Store I|2022|\n","|         10| 30/10/2022|              1800| Purchase J| Store J|2022|\n","|         11|  5/11/2022|              2200| Purchase K| Store K|2022|\n","|         12| 10/12/2022|               900| Purchase L| Store L|2022|\n","|         13|  15/1/2023|              4800| Purchase M| Store M|2023|\n","|         14|  20/2/2023|               300| Purchase N| Store N|2023|\n","|         15|  25/3/2023|              4200| Purchase O| Store O|2023|\n","|         16|  30/4/2023|              2600| Purchase P| Store P|2023|\n","|         17|   5/5/2023|               700| Purchase Q| Store Q|2023|\n","|         18|  10/6/2023|              1500| Purchase R| Store R|2023|\n","|         19|  15/7/2023|              3200| Purchase S| Store S|2023|\n","|         20|  20/8/2023|              1000| Purchase T| Store T|2023|\n","+-----------+-----------+------------------+-----------+--------+----+\n","only showing top 20 rows\n","\n","+-----------+----------------+-----------------+-------+-------+\n","|customer_id|transaction_date|transaction_value|  notes|quarter|\n","+-----------+----------------+-----------------+-------+-------+\n","|          1|        1/1/2022|             1500| Note 1|      1|\n","|          2|       15/2/2022|             2000| Note 2|      1|\n","|          3|       20/3/2022|             1000| Note 3|      1|\n","|          4|       10/4/2022|             2500| Note 4|      2|\n","|          5|        5/5/2022|             1800| Note 5|      2|\n","|          6|       10/6/2022|             1200| Note 6|      2|\n","|          7|       15/7/2022|              700| Note 7|      3|\n","|          8|       20/8/2022|             3000| Note 8|      3|\n","|          9|       25/9/2022|              600| Note 9|      3|\n","|         10|      30/10/2022|             1200|Note 10|      4|\n","|         11|       5/11/2022|             1500|Note 11|      4|\n","|         12|      10/12/2022|              800|Note 12|      4|\n","|         13|       15/1/2023|             2000|Note 13|      1|\n","|         14|       20/2/2023|              700|Note 14|      1|\n","|         15|       25/3/2023|             1800|Note 15|      1|\n","|         16|       30/4/2023|             1000|Note 16|      2|\n","|         17|        5/5/2023|              400|Note 17|      2|\n","|         18|       10/6/2023|             1500|Note 18|      2|\n","|         19|       15/7/2023|             3000|Note 19|      3|\n","|         20|       20/8/2023|              600|Note 20|      3|\n","+-----------+----------------+-----------------+-------+-------+\n","only showing top 20 rows\n","\n"]}],"source":["#Rename df1 column amount to transaction_amount\n","df1=df1.withColumnRenamed('amount','transaction_amount')\n","df1.show()\n","\n","#Rename df2 column value to transaction_value\n","df2=df2.withColumnRenamed('value','transaction_value')\n","df2.show()\n"]},{"cell_type":"markdown","id":"7360228f-e4ff-4d56-8550-8d51862f65b6","metadata":{},"source":["#### Drop unnecessary columns\n","\n","\n","\n"]},{"cell_type":"code","execution_count":25,"id":"003d4f6a-d80d-4706-91cc-32cb92309dba","metadata":{},"outputs":[],"source":["#Drop columns description and location from df1\n","df1=df1.drop('description','location')\n","\n","#Drop column notes from df2\n","df2=df2.drop('notes')\n"]},{"cell_type":"markdown","id":"a3912d67-1af2-4b81-965d-4eeba66e375a","metadata":{},"source":["#### Join dataframes based on a common column\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":29,"id":"46a05016-2a55-48f6-8967-44b06ae40806","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+------------------+----+----------------+-----------------+-------+\n","|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n","+-----------+-----------+------------------+----+----------------+-----------------+-------+\n","|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|      1|\n","|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|      1|\n","|          3|  20/3/2022|               800|2022|       20/3/2022|             1000|      1|\n","|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|      2|\n","|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|      2|\n","|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200|      2|\n","|          7|  15/7/2022|               200|2022|       15/7/2022|              700|      3|\n","|          8|  20/8/2022|              3500|2022|       20/8/2022|             3000|      3|\n","|          9|  25/9/2022|               700|2022|       25/9/2022|              600|      3|\n","|         10| 30/10/2022|              1800|2022|      30/10/2022|             1200|      4|\n","|         11|  5/11/2022|              2200|2022|       5/11/2022|             1500|      4|\n","|         12| 10/12/2022|               900|2022|      10/12/2022|              800|      4|\n","|         13|  15/1/2023|              4800|2023|       15/1/2023|             2000|      1|\n","|         14|  20/2/2023|               300|2023|       20/2/2023|              700|      1|\n","|         15|  25/3/2023|              4200|2023|       25/3/2023|             1800|      1|\n","|         16|  30/4/2023|              2600|2023|       30/4/2023|             1000|      2|\n","|         17|   5/5/2023|               700|2023|        5/5/2023|              400|      2|\n","|         18|  10/6/2023|              1500|2023|       10/6/2023|             1500|      2|\n","|         19|  15/7/2023|              3200|2023|       15/7/2023|             3000|      3|\n","|         20|  20/8/2023|              1000|2023|       20/8/2023|              600|      3|\n","+-----------+-----------+------------------+----+----------------+-----------------+-------+\n","only showing top 20 rows\n","\n"]}],"source":["#join df1 and df2 based on common column customer_id\n","joined_df=df1.join(df2,on='customer_id',how='inner')\n","joined_df.show()"]},{"cell_type":"markdown","id":"d527707b-9d6e-4249-8cd2-75cf05314e4b","metadata":{},"source":["####  Filter data based on a condition\n","\n","\n"]},{"cell_type":"code","execution_count":33,"id":"00f48f26-b464-4a6e-9d07-30f9828ad74e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+------------------+----+----------------+-----------------+-------+\n","|customer_id|date_column|transaction_amount|year|transaction_date|transaction_value|quarter|\n","+-----------+-----------+------------------+----+----------------+-----------------+-------+\n","|          1|   1/1/2022|              5000|2022|        1/1/2022|             1500|      1|\n","|          2|  15/2/2022|              1200|2022|       15/2/2022|             2000|      1|\n","|          4|  10/4/2022|              3000|2022|       10/4/2022|             2500|      2|\n","|          5|   5/5/2022|              6000|2022|        5/5/2022|             1800|      2|\n","|          6|  10/6/2022|              4500|2022|       10/6/2022|             1200|      2|\n","|          8|  20/8/2022|              3500|2022|       20/8/2022|             3000|      3|\n","|         10| 30/10/2022|              1800|2022|      30/10/2022|             1200|      4|\n","|         11|  5/11/2022|              2200|2022|       5/11/2022|             1500|      4|\n","|         13|  15/1/2023|              4800|2023|       15/1/2023|             2000|      1|\n","|         15|  25/3/2023|              4200|2023|       25/3/2023|             1800|      1|\n","|         16|  30/4/2023|              2600|2023|       30/4/2023|             1000|      2|\n","|         18|  10/6/2023|              1500|2023|       10/6/2023|             1500|      2|\n","|         19|  15/7/2023|              3200|2023|       15/7/2023|             3000|      3|\n","|         21|  25/9/2023|              5500|2023|       25/9/2023|             2500|      3|\n","|         22| 30/10/2023|              1200|2023|      30/10/2023|              700|      4|\n","|         24| 10/12/2023|              2400|2023|      10/12/2023|             1200|      4|\n","|         25|  15/1/2024|              1800|2024|       15/1/2024|              800|      1|\n","|         27|  25/3/2024|              4200|2024|       25/3/2024|             1800|      1|\n","|         28|  30/4/2024|              2600|2024|       30/4/2024|             1000|      2|\n","|         30|  10/6/2024|              1500|2024|       10/6/2024|             1500|      2|\n","+-----------+-----------+------------------+----+----------------+-----------------+-------+\n","only showing top 20 rows\n","\n"]}],"source":["# filter the dataframe for transaction amount > 1000\n","\n","filtered_df= joined_df.filter('transaction_amount > 1000')\n","filtered_df.show()\n"]},{"cell_type":"markdown","id":"4b0fe619-bda7-412d-9a90-491dcc9be04a","metadata":{},"source":["####  Aggregate data by customer\n","\n"]},{"cell_type":"code","execution_count":45,"id":"d70b0054-b567-402c-8001-85724a78e7d0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+\n","|customer_id|total_amount|\n","+-----------+------------+\n","|         31|        3200|\n","|         85|        1800|\n","|         78|        1500|\n","|         34|        1200|\n","|         81|        5500|\n","|         28|        2600|\n","|         76|        2600|\n","|         27|        4200|\n","|         91|        3200|\n","|         22|        1200|\n","|         93|        5500|\n","|          1|        5000|\n","|         52|        2600|\n","|         13|        4800|\n","|          6|        4500|\n","|         16|        2600|\n","|         40|        2600|\n","|         94|        1200|\n","|         57|        5500|\n","|         54|        1500|\n","+-----------+------------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import sum\n","# group by customer_id and aggregate the sum of transaction amount\n","total_amount_per_person=filtered_df.groupBy(['customer_id']).agg({'transaction_amount':'sum'}) #method 1\n","total_amount_per_person=filtered_df.groupBy(['customer_id']).agg(sum('transaction_amount').alias('total_amount')) #method 1\n","\n","#display the result\n","total_amount_per_person.show()\n","\n"]},{"cell_type":"markdown","id":"b89f3e16-ae15-4033-98c8-e213101152f6","metadata":{},"source":["#### Write the result to a HDFS in Local\n","\n"]},{"cell_type":"code","execution_count":63,"id":"e940b0e2-6937-45c1-acb9-fca99d331720","metadata":{},"outputs":[],"source":["# Write total_amount_per_customer to a HDFS in Local\n","\n","total_amount_per_person.write.mode(\"overwrite\").option(\"header\", \"true\").csv('hdfs://localhost:8020/user/hive/warehouse/bankdata/total_amount_per_person.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reading Data for trial Puposes\n","df = spark.read.csv('hdfs://localhost:8020/user/hive/warehouse/bankdata/total_amount_per_person.csv/filename',inferSchema=True,header=True)\n"]},{"cell_type":"markdown","id":"1eedc153-059b-487c-9b75-df4543b9b50b","metadata":{},"source":["#### Write the filtered data to HDFS in Local\n","\n"]},{"cell_type":"code","execution_count":65,"id":"f1bfc945-505c-428e-987e-04db57662563","metadata":{},"outputs":[],"source":["#Write filtered_df to HDFS in Local\n","\n","filtered_df.write.mode(\"overwrite\").option(\"header\", \"true\").parquet('hdfs://localhost:8020/user/hive/warehouse/bankdata/filtered_df.parquet')"]},{"cell_type":"markdown","id":"b2e5acd7-65ff-48d3-a394-4356dcaa51b7","metadata":{},"source":["#### Add a new column based on a condition\n","\n"]},{"cell_type":"code","execution_count":75,"id":"fbfd05f6-a233-429b-a34a-d4132fe0e807","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+-----------+------------------+----+---------+\n","|customer_id|date_column|transaction_amount|year|indicator|\n","+-----------+-----------+------------------+----+---------+\n","|          1|   1/1/2022|              5000|2022|       No|\n","|          2|  15/2/2022|              1200|2022|       No|\n","|          3|  20/3/2022|               800|2022|       No|\n","|          4|  10/4/2022|              3000|2022|       No|\n","|          5|   5/5/2022|              6000|2022|      Yes|\n","|          6|  10/6/2022|              4500|2022|       No|\n","|          7|  15/7/2022|               200|2022|       No|\n","|          8|  20/8/2022|              3500|2022|       No|\n","|          9|  25/9/2022|               700|2022|       No|\n","|         10| 30/10/2022|              1800|2022|       No|\n","|         11|  5/11/2022|              2200|2022|       No|\n","|         12| 10/12/2022|               900|2022|       No|\n","|         13|  15/1/2023|              4800|2023|       No|\n","|         14|  20/2/2023|               300|2023|       No|\n","|         15|  25/3/2023|              4200|2023|       No|\n","|         16|  30/4/2023|              2600|2023|       No|\n","|         17|   5/5/2023|               700|2023|       No|\n","|         18|  10/6/2023|              1500|2023|       No|\n","|         19|  15/7/2023|              3200|2023|       No|\n","|         20|  20/8/2023|              1000|2023|       No|\n","+-----------+-----------+------------------+----+---------+\n","only showing top 20 rows\n","\n"]}],"source":["# Add new column with value indicating whether transaction amount is > 5000 or not\n","from pyspark.sql.functions import when\n","\n","df1=df1.withColumn('indicator',when(df1['transaction_amount']>5000,'Yes').otherwise(\"No\"))\n","\n","df1.show()"]},{"cell_type":"markdown","id":"30b7f4fe-4d91-4321-99c6-e2fbd401d3f0","metadata":{},"source":["#### Calculate the average transaction value per quarter\n","\n"]},{"cell_type":"code","execution_count":73,"id":"c863a222-7d9c-4396-8556-0bf9b040773c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|quarter|     avg_trans_val|\n","+-------+------------------+\n","|      1| 1111.111111111111|\n","|      3|1958.3333333333333|\n","|      4| 816.6666666666666|\n","|      2|            1072.0|\n","+-------+------------------+\n","\n"]}],"source":["from pyspark.sql.functions import avg\n","\n","#calculate the average transaction value for each quarter in df2\n","average_value_per_quarter=df2.groupby(\"quarter\").agg(avg('transaction_value').alias('avg_trans_val'))\n","\n","#show the average transaction value for each quarter in df2    \n","average_value_per_quarter.show()\n"]},{"cell_type":"markdown","id":"27388f97-5a6a-4d5a-a908-b8ef596930ba","metadata":{},"source":["####  Write the result to a HDFS in Local\n","\n"]},{"cell_type":"code","execution_count":74,"id":"47a24230-6e72-45b6-a008-6102464d0e7a","metadata":{},"outputs":[],"source":["#Write average_value_per_quarter to a HDFS in Local\n","average_value_per_quarter.write.mode(\"overwrite\").option(\"header\", \"true\").csv('hdfs://localhost:8020/user/hive/warehouse/bankdata/quarterly_averages')\n"]},{"cell_type":"markdown","id":"8015e825-e1b2-4115-9698-fe5d09bc2479","metadata":{},"source":["#### Calculate the total transaction value per year\n"]},{"cell_type":"code","execution_count":78,"id":"8932e47d-f850-47a3-9a0d-ae68e4233a33","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+--------------+\n","|year|total_per_year|\n","+----+--------------+\n","|2025|         25700|\n","|2027|         25700|\n","|2023|         28100|\n","|2022|         29800|\n","|2026|         25700|\n","|2029|         25700|\n","|2030|          9500|\n","|2028|         25700|\n","|2024|         25700|\n","+----+--------------+\n","\n"]}],"source":["# calculate the total transaction value for each year in df1.\n","total_value_per_year=df1.groupby('year').agg(sum('transaction_amount').alias('total_per_year'))\n","\n","# show the total transaction value for each year in df1.\n","total_value_per_year.show()\n"]},{"cell_type":"markdown","id":"ab365c1a-4624-45e0-8eba-f51108a1f7ec","metadata":{},"source":["#### Task 15: Write the result to HDFS in Local\n","\n","\n"]},{"cell_type":"code","execution_count":79,"id":"f9a9e538-d9d0-4682-bbf2-89f7e1ebd325","metadata":{},"outputs":[],"source":["#Write total_value_per_year to HDFS in the CSV format\n","\n","total_value_per_year.write.mode(\"overwrite\").option(\"header\", \"true\").csv('hdfs://localhost:8020/user/hive/warehouse/bankdata/total_value_per_year')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
