# PySpark and HDFS Data Transformation and Integration Project

This project focuses on data transformation and integration using PySpark. It involves working with two datasets, performing various transformations such as adding columns, renaming columns, dropping unnecessary columns, joining DataFrames, and finally, writing the results into a Hive warehouse and an HDFS file system.

## Introduction

The project utilizes two datasets representing different aspects of customer activities:

1. **Transactions Dataset**: This dataset contains information about financial transactions, including customer ID, transaction date, transaction value, and additional notes.

2. **Purchases Dataset**: This dataset represents purchase transactions made by customers, including customer ID, purchase date, purchase amount, purchase description, and purchase location.

The goal of the project is to integrate and transform these datasets using PySpark to derive insights about customer behavior and spending patterns.

## Requirements

- Python
- Apache Spark
- Hive (for writing to Hive warehouse)
- HDFS (for writing to HDFS file system)
- Hadoop installed locally on your VM

## Contributing

Contributions are welcome! Please create a new branch, make your changes, and submit a pull request and if you like it please star it.
